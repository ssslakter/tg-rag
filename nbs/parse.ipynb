{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as l\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "from tg_rag.config import Config\n",
    "from tg_rag.utils import init_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = l.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_book(url):\n",
    "    log.info(f\"Downloading the book from {url}\")\n",
    "    response = requests.get(url)\n",
    "    myzip = ZipFile(BytesIO(response.content))\n",
    "    file = myzip.namelist()[0]\n",
    "    text = myzip.open(file).read().decode(\"windows-1251\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def is_dialog(text):\n",
    "    return text.startswith(\"-\") or text.startswith(\"–\") or text.startswith(\"—\")\n",
    "\n",
    "def merge_dialogs(paragraphs, max_words = 500):\n",
    "    merged_chunk = []\n",
    "    result = []\n",
    "    for p in paragraphs:\n",
    "        if is_dialog(p) and count_words(\" \".join(merged_chunk)) < max_words: \n",
    "            merged_chunk.append(p)\n",
    "            continue\n",
    "        if merged_chunk:\n",
    "            result.append(\" \".join(merged_chunk))\n",
    "            merged_chunk = []\n",
    "            \n",
    "        if is_dialog(p): merged_chunk.append(p)\n",
    "        else: result.append(p)   \n",
    "    return result\n",
    "\n",
    "def merge_short_paragraphs(paragraphs, min_words = 50, max_words = 500):\n",
    "    merged_chunk = []\n",
    "    result = []\n",
    "    for p in paragraphs:\n",
    "        if count_words(\" \".join(merged_chunk)) >= max_words:\n",
    "            result.append(\" \".join(merged_chunk))\n",
    "            merged_chunk = []\n",
    "    \n",
    "        if count_words(p) < min_words: merged_chunk.append(p)\n",
    "        else:\n",
    "            result.append(\" \".join(merged_chunk+[p]))\n",
    "            merged_chunk = []\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_book(book):\n",
    "    book = book[book.find(\"ЧАСТЬ ПЕРВАЯ\"):]\n",
    "    paragraphs = [t for t in book.split(\"\\r\\n\") if t!='']\n",
    "    paragraphs = merge_dialogs(paragraphs, 100)     \n",
    "    paragraphs = merge_short_paragraphs(paragraphs, 80, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "b = download_book(cfg.book_url)\n",
    "paragraphs = parse_book(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_client, index_name):\n",
    "    log.info(f\"Creating index {index_name}\")\n",
    "    mappings = {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"term_vector\": \"yes\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    es_client.indices.create(index=index_name, mappings=mappings)\n",
    "    log.info(f\"Index {index_name} created.\")\n",
    "\n",
    "\n",
    "def index_paragraphs(es, index_name, paragraphs):\n",
    "    \"\"\"Index each paragraph into the specified Elasticsearch index.\"\"\"\n",
    "    log.info(\"Indexing paragraphs...\")\n",
    "    for i, paragraph in enumerate(progress_bar(paragraphs)):\n",
    "        doc = {'text': paragraph}\n",
    "        es.index(index=index_name, id=i, document=doc)\n",
    "    es.indices.refresh(index=index_name)\n",
    "    log.info(\"Indexing completed\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    init_logger(log.name)\n",
    "    init_logger(name=\"elastic_transport.transport\", level=log.ERROR)\n",
    "    cfg = Config()\n",
    "\n",
    "    paragraphs = download_book(cfg.book_url)\n",
    "\n",
    "    log.info(f\"Connecting to {cfg.es_creds}@{cfg.es_url}\")\n",
    "    es = Elasticsearch([cfg.es_url], basic_auth=cfg.es_creds, verify_certs=False)\n",
    "\n",
    "    create_index(es, \"my_index\")\n",
    "    index_paragraphs(es, \"my_index\", paragraphs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
